{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from datosgobmx import client\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tests the data from aqip compared to cdmx\n",
    "def t_test(df_aqip, df_mx):\n",
    "    \"\"\"Calculates t test and p value for two air quality dataframes.\n",
    "\n",
    "    Args:\n",
    "        df_aqip (dataframe): dataframe with concentrations for mexican cities from air quality index project.\n",
    "        df_mx (dataframe): dafaframe with concentrations from stations for mexican cities.\n",
    "\n",
    "    Returns:\n",
    "        [tuple]: tuple with t and p values for the compared dataframes.\n",
    "    \"\"\"\n",
    "\n",
    "    t, p = stats.ttest_ind(df_mx, df_aqip, equal_var=False)\n",
    "    \n",
    "    return (t, p)\n",
    "\n",
    "def pollutant(p):\n",
    "    \"\"\"Function that returns a str with a pollutant.\n",
    "\n",
    "    Args:\n",
    "        p (int): values from 0 to 5 for list place.\n",
    "\n",
    "    Returns:\n",
    "        str: pollutant.\n",
    "    \"\"\"\n",
    "    #Parametros de contaminantes\n",
    "    param = ['CO','NO2', 'O3','PM10','SO2']\n",
    "    return (param[p])\n",
    "\n",
    "def aqip_mx(city):\n",
    "\n",
    "    \"\"\"Merges dataframes from csv with air quality daily median concentrations for\n",
    "        air quality index project and mexican cities stations\n",
    "    \"\"\"\n",
    "\n",
    "    dir_pcs = '../data/processed/'\n",
    "    dir_pcs_aqip = '../data/processed/aqip/'\n",
    "\n",
    "    city_dict = {'gdl':'Guadalajara', 'cmdx':'Mexico City'}\n",
    "\n",
    "    if not os.path.isdir(dir_pcs+'aqip_'+city): \n",
    "        os.mkdir(dir_pcs+city) \n",
    "\n",
    "    dir_pcs_cat = dir_pcs+'aqip_'+city #Directory to save concatenation\n",
    "\n",
    "    mx = pd.read_csv(dir_pcs+city+'/'+'median_res_2017-2020.csv')\n",
    "    aqip = pd.read_csv(dir_pcs_aqip +'MX_2015_2020.csv', index_col=[0])\n",
    "\n",
    "    aqip = aqip.loc[city_dict[city]]\n",
    "\n",
    "    compare = pd.merge(aqip, mx, how='inner', left_on=['Specie','Date'], right_on=['PARAM','FECHA'])\n",
    "\n",
    "    compare = compare.drop(columns=['count','min','max','median','variance','PARAM','FECHA']).rename(columns={'Specie':'Contaminante',\n",
    "                                                                                                   'Date':'Fecha',\n",
    "                                                                                                   'c_median':'aqip_median',\n",
    "                                                                                                   '0':'mx_median'})\n",
    "    compare.to_csv(dir_pcs_cat +city+'_AQIP.csv')\n",
    "\n",
    "    \n",
    "def data_valid(city):\n",
    "    \"\"\"Function that compares if the air quality data from mexican monitoring stations and\n",
    "        the air quality index project are statistically different.\n",
    "\n",
    "    Args:\n",
    "        city (str): city code for the city to by analyzed.\n",
    "    \"\"\"\n",
    "\n",
    "    dir_pcs = '../data/processed/'\n",
    "    dir_pcs_cat = dir_pcs+'aqip_'+city #Directory to save concatenation\n",
    "\n",
    "    valid_check = pd.read_csv(dir_pcs_cat +city+'_AQIP.csv')\n",
    "    \n",
    "    df_aqip = valid_check[['Contaminante','aqip_median']]\n",
    "    df_mx = valid_check[['Contaminante','mx_median']]\n",
    "    \n",
    "    df_aqip['Contaminante']\n",
    "    \n",
    "    for i in range(6):\n",
    "        \n",
    "        #print (df_aqip[df_aqip['Contaminante']==c].drop(columns=['Contaminante']))\n",
    "        \n",
    "        t,p = t_test(df_aqip[df_aqip['Contaminante']==pollutant(i)].drop(columns=['Contaminante']),\n",
    "                    df_mx[df_mx['Contaminante']==pollutant(i)].drop(columns=['Contaminante']))\n",
    "\n",
    "        print ('For: '+pollutant(i)+' t value is: '+str(t)+' and p value is: '+str(p))\n",
    "\n",
    "def airquality_average(city):\n",
    "    \"\"\"Function that creates separate csv for the first four months of the yearly data available.\n",
    "\n",
    "    Args:\n",
    "        data_csv (str): string containing the directory and name of the csv file\n",
    "\n",
    "    Returns:\n",
    "        csv: individual csv for each pollutant with the average data by week of the first four months of the yearly data available.\n",
    "    \"\"\"\n",
    "    dir_pcs_mx = '../data/processed/'+city+'/'\n",
    "    #data_csv = pd.read_csv(dir_pcs_mx+city+'_2017-2019.csv').set_index(['FECHA','PARAM'])\n",
    "    data_csv = dir_pcs_mx+city+'_2017-2019.csv'\n",
    "    \n",
    "    if city == 'cdmx':\n",
    "        data_csv = '../data/processed/'+city+'/'+city+'_2017-2020_filtered.csv'\n",
    "    \n",
    "    month = [1,2,3,4]\n",
    "    \n",
    "    data_bydate = pd.read_csv(data_csv).set_index(['FECHA','PARAM']).groupby(level=('FECHA','PARAM')).mean().reset_index()\n",
    "    \n",
    "    data_bydate['FECHA'] = pd.to_datetime(data_bydate['FECHA'])\n",
    "    \n",
    "    for m in month:\n",
    "        if m == 1: \n",
    "            filter_month=data_bydate[data_bydate['FECHA'].dt.month==m]\n",
    "\n",
    "        else:\n",
    "\n",
    "            month_tmp = data_bydate[data_bydate['FECHA'].dt.month==m]\n",
    "\n",
    "\n",
    "            filter_month = filter_month.append(month_tmp)\n",
    "    \n",
    "    for i in range(5):\n",
    "        \n",
    "        data_bydateParam = filter_month[filter_month['PARAM']==pollutant(i)].set_index('FECHA')\n",
    "        \n",
    "        data_bydateParam = data_bydateParam.rolling(7, min_periods=1).mean()\n",
    "        \n",
    "        if pollutant(i)!= 'PM10':\n",
    "            data_bydateParam = data_bydateParam*1000\n",
    "        \n",
    "        data_bydateParam.to_csv(data_csv[:-4]+'_'+pollutant(i)+'.csv')\n",
    "        \n",
    "    #return (data_bydateParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality_average('gdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = airquality_average('gdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATM</th>\n",
       "      <th>AGU</th>\n",
       "      <th>LDO</th>\n",
       "      <th>MIR</th>\n",
       "      <th>CEN</th>\n",
       "      <th>OBL</th>\n",
       "      <th>PIN</th>\n",
       "      <th>TLA</th>\n",
       "      <th>SFE</th>\n",
       "      <th>VAL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FECHA</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-01</th>\n",
       "      <td>20.416667</td>\n",
       "      <td>20.916667</td>\n",
       "      <td>16.708333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.208333</td>\n",
       "      <td>25.291667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.291667</td>\n",
       "      <td>24.583333</td>\n",
       "      <td>15.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>25.104167</td>\n",
       "      <td>20.458333</td>\n",
       "      <td>28.083333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.354167</td>\n",
       "      <td>29.604167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.041667</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>15.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-03</th>\n",
       "      <td>27.013889</td>\n",
       "      <td>21.083333</td>\n",
       "      <td>29.513889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.513889</td>\n",
       "      <td>32.236111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.013889</td>\n",
       "      <td>31.888889</td>\n",
       "      <td>15.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-04</th>\n",
       "      <td>27.531250</td>\n",
       "      <td>21.843750</td>\n",
       "      <td>27.427083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.697917</td>\n",
       "      <td>31.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.510417</td>\n",
       "      <td>34.322917</td>\n",
       "      <td>16.364583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-05</th>\n",
       "      <td>28.333333</td>\n",
       "      <td>23.550000</td>\n",
       "      <td>27.891667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.841667</td>\n",
       "      <td>33.850000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.191667</td>\n",
       "      <td>35.450000</td>\n",
       "      <td>15.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-26</th>\n",
       "      <td>20.108333</td>\n",
       "      <td>23.857143</td>\n",
       "      <td>13.898810</td>\n",
       "      <td>19.857143</td>\n",
       "      <td>30.038690</td>\n",
       "      <td>20.732143</td>\n",
       "      <td>52.607143</td>\n",
       "      <td>41.809524</td>\n",
       "      <td>39.767857</td>\n",
       "      <td>15.303571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-27</th>\n",
       "      <td>19.560714</td>\n",
       "      <td>24.363095</td>\n",
       "      <td>13.602743</td>\n",
       "      <td>20.363095</td>\n",
       "      <td>34.252976</td>\n",
       "      <td>19.922619</td>\n",
       "      <td>53.529449</td>\n",
       "      <td>41.029762</td>\n",
       "      <td>39.619963</td>\n",
       "      <td>15.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-28</th>\n",
       "      <td>20.310714</td>\n",
       "      <td>25.172619</td>\n",
       "      <td>11.965580</td>\n",
       "      <td>21.172619</td>\n",
       "      <td>34.806548</td>\n",
       "      <td>18.464286</td>\n",
       "      <td>55.159357</td>\n",
       "      <td>40.797619</td>\n",
       "      <td>41.115201</td>\n",
       "      <td>15.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-29</th>\n",
       "      <td>20.215476</td>\n",
       "      <td>24.958333</td>\n",
       "      <td>11.507246</td>\n",
       "      <td>20.958333</td>\n",
       "      <td>35.122024</td>\n",
       "      <td>18.357143</td>\n",
       "      <td>58.532895</td>\n",
       "      <td>41.488095</td>\n",
       "      <td>41.775916</td>\n",
       "      <td>15.565476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>19.721429</td>\n",
       "      <td>23.464286</td>\n",
       "      <td>11.769151</td>\n",
       "      <td>19.464286</td>\n",
       "      <td>34.497024</td>\n",
       "      <td>18.910714</td>\n",
       "      <td>55.309818</td>\n",
       "      <td>40.333333</td>\n",
       "      <td>43.460440</td>\n",
       "      <td>15.904762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ATM        AGU        LDO        MIR        CEN        OBL  \\\n",
       "FECHA                                                                          \n",
       "2017-01-01  20.416667  20.916667  16.708333        NaN  18.208333  25.291667   \n",
       "2017-01-02  25.104167  20.458333  28.083333        NaN  18.354167  29.604167   \n",
       "2017-01-03  27.013889  21.083333  29.513889        NaN  18.513889  32.236111   \n",
       "2017-01-04  27.531250  21.843750  27.427083        NaN  18.697917  31.666667   \n",
       "2017-01-05  28.333333  23.550000  27.891667        NaN  18.841667  33.850000   \n",
       "...               ...        ...        ...        ...        ...        ...   \n",
       "2019-04-26  20.108333  23.857143  13.898810  19.857143  30.038690  20.732143   \n",
       "2019-04-27  19.560714  24.363095  13.602743  20.363095  34.252976  19.922619   \n",
       "2019-04-28  20.310714  25.172619  11.965580  21.172619  34.806548  18.464286   \n",
       "2019-04-29  20.215476  24.958333  11.507246  20.958333  35.122024  18.357143   \n",
       "2019-04-30  19.721429  23.464286  11.769151  19.464286  34.497024  18.910714   \n",
       "\n",
       "                  PIN        TLA        SFE        VAL  \n",
       "FECHA                                                   \n",
       "2017-01-01        NaN  31.291667  24.583333  15.666667  \n",
       "2017-01-02        NaN  35.041667  28.500000  15.250000  \n",
       "2017-01-03        NaN  37.013889  31.888889  15.333333  \n",
       "2017-01-04        NaN  39.510417  34.322917  16.364583  \n",
       "2017-01-05        NaN  40.191667  35.450000  15.700000  \n",
       "...               ...        ...        ...        ...  \n",
       "2019-04-26  52.607143  41.809524  39.767857  15.303571  \n",
       "2019-04-27  53.529449  41.029762  39.619963  15.791667  \n",
       "2019-04-28  55.159357  40.797619  41.115201  15.607143  \n",
       "2019-04-29  58.532895  41.488095  41.775916  15.565476  \n",
       "2019-04-30  55.309818  40.333333  43.460440  15.904762  \n",
       "\n",
       "[360 rows x 10 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df*10000\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('geo_env': conda)",
   "language": "python",
   "name": "python38364bitgeoenvconda2cb6af09078d46c89f7c036ca6304ba0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
